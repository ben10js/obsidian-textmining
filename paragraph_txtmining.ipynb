{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1deoAqZpzlv",
        "outputId": "0cf7665b-09d2-449f-9de6-2f58e5748d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/MetaCogAI/the-verdict.txt'  # 경로 바꾸기\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 여러 md 글 파일이 하나 있으면 문서 분리(예: 헤더 기준 ### 등)\n",
        "documents = text.split('\\n=====\\n')  # Obsidian에서 문서 구분 텍스트가 있으면 활용\n"
      ],
      "metadata": {
        "id": "k0keTxoMqhe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "2Mm43jkSrnn5",
        "outputId": "a31b38eb-d57f-47f1-addc-503b6592ffbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"아래는 여행 관련 템플릿 노트를 메타데이터 형식으로 작성한 예시입니다. 이 템플릿은 Obsidian에서 활용할 수 있도록 설계되었으며, 필요한 내용을 추가하고 체계적으로 정리할 수 있습니다. Travel Template - 노트 제목은 여행지(공간) 기준으로 작성한다. Trip Name: Destination: Travel Dates: Start Date - End Date Travel Companions: (Names, relationship, or group info) Purpose of Travel: (e.g., leisure, business, family visit, etc.) Last Updated: 1. Travel Itineraries Overview: 하루 단위의 종합 일정 Day 1: 활동장소 Day 2: 활동장소 ... 2. Packing Lists Essentials: Passport, Visa, National ID Travel Insurance Documents Electronics: Phone, Charger, Adapter, Power Bank Toiletries: Toothbrush, Shampoo, etc. Medications: Prescriptions, First Aid Kit Clothing: (기후에 따른 옷 추가) Other: Snacks, Entertainment, etc. 3. Travel Insurance Provider: Policy Number: Coverage Details: Medical: YesNo Trip Cancellation: YesNo Baggage Loss: YesNo Contact Information: 4. Accommodation Bookings Accommodation 1: Name: Address: Check-InOut: Booking Reference: Accommodation 2: Name: Address: Check-InOut: Booking Reference: 5. Flight Details Flight 1: Airline: Flight Number: Departure: DateTime, Location Arrival: DateTime, Location Flight 2: Airline: Flight Number: Departure: DateTime, Location Arrival: DateTime, Location 6. Local Attractions Must-See: Attraction 1: Description, Tickets Required? YN Attraction 2: Description, Tickets Required? YN Optional: 7. Travel Budgeting Planned Budget: Flights: Accommodation: Food: Attractions: Miscellaneous: Actual Spending: (업데이트 시 입력) 8. Travel Documents Passport: Number, Expiry Date Visa: Type, Expiry Date National ID: Driver's License: Other: Refugee Travel Document: Advance Parole Document: Reentry Permit: Temporary Protected Status Travel Authorization: 9. Notes 기타 메모 작성 여행 중 느낀 점, 사진 링크 등 기록 이 템플릿은 여행을 시작하기 전에 정보를 입력하고, 여행 중 및 후에 업데이트하며 활용할 수 있도록 구성되어 있습니다. Obsidian에서 wikilinks로 다른 관련 노트와 연결하면 더욱 유용합니다!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = documents[4]"
      ],
      "metadata": {
        "id": "SKZWClFzr69S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_test = documents[100]"
      ],
      "metadata": {
        "id": "R41aVFWHsmqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def tokenize_english(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [t.lemma_.lower() for t in doc if t.is_alpha and not t.is_stop]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "_GgcwKv7rjU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_english(eng_test)"
      ],
      "metadata": {
        "id": "658Cefgarra0",
        "outputId": "7613bb9a-d646-4659-e340-73bc600513e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아래는',\n",
              " '여행',\n",
              " '관련',\n",
              " '템플릿',\n",
              " '노트를',\n",
              " '메타데이터',\n",
              " '형식으로',\n",
              " '작성한',\n",
              " '예시입니다',\n",
              " '이',\n",
              " '템플릿은',\n",
              " 'obsidian에서',\n",
              " '활용할',\n",
              " '수',\n",
              " '있도록',\n",
              " '설계되었으며',\n",
              " '필요한',\n",
              " '내용을',\n",
              " '추가하고',\n",
              " '체계적으로',\n",
              " '정리할',\n",
              " '수',\n",
              " '있습니다',\n",
              " 'travel',\n",
              " 'template',\n",
              " '노트',\n",
              " '제목은',\n",
              " '기준으로',\n",
              " '작성한다',\n",
              " 'trip',\n",
              " 'destination',\n",
              " 'travel',\n",
              " 'date',\n",
              " 'start',\n",
              " 'date',\n",
              " 'end',\n",
              " 'date',\n",
              " 'travel',\n",
              " 'companions',\n",
              " 'name',\n",
              " 'relationship',\n",
              " 'group',\n",
              " 'info',\n",
              " 'purpose',\n",
              " 'travel',\n",
              " 'leisure',\n",
              " 'business',\n",
              " 'family',\n",
              " 'visit',\n",
              " 'etc',\n",
              " 'update',\n",
              " 'travel',\n",
              " 'itineraries',\n",
              " 'overview',\n",
              " '하루',\n",
              " '단위의',\n",
              " '종합',\n",
              " '일정',\n",
              " 'day',\n",
              " '활동장소',\n",
              " 'day',\n",
              " '활동장소',\n",
              " 'packing',\n",
              " 'lists',\n",
              " 'essentials',\n",
              " 'passport',\n",
              " 'visa',\n",
              " 'national',\n",
              " 'id',\n",
              " 'travel',\n",
              " 'insurance',\n",
              " 'documents',\n",
              " 'electronics',\n",
              " 'phone',\n",
              " 'charger',\n",
              " 'adapter',\n",
              " 'power',\n",
              " 'bank',\n",
              " 'toiletries',\n",
              " 'toothbrush',\n",
              " 'shampoo',\n",
              " 'etc',\n",
              " 'medication',\n",
              " 'prescription',\n",
              " 'aid',\n",
              " 'kit',\n",
              " 'clothing',\n",
              " '기후에',\n",
              " '따른',\n",
              " '옷',\n",
              " '추가',\n",
              " 'snacks',\n",
              " 'entertainment',\n",
              " 'etc',\n",
              " 'travel',\n",
              " 'insurance',\n",
              " 'provider',\n",
              " 'policy',\n",
              " 'number',\n",
              " 'coverage',\n",
              " 'detail',\n",
              " 'medical',\n",
              " 'yesno',\n",
              " 'trip',\n",
              " 'cancellation',\n",
              " 'yesno',\n",
              " 'baggage',\n",
              " 'loss',\n",
              " 'yesno',\n",
              " 'contact',\n",
              " 'information',\n",
              " 'accommodation',\n",
              " 'booking',\n",
              " 'accommodation',\n",
              " 'address',\n",
              " 'check',\n",
              " 'inout',\n",
              " 'booking',\n",
              " 'reference',\n",
              " 'accommodation',\n",
              " 'address',\n",
              " 'check',\n",
              " 'inout',\n",
              " 'booking',\n",
              " 'reference',\n",
              " 'flight',\n",
              " 'details',\n",
              " 'flight',\n",
              " 'airline',\n",
              " 'flight',\n",
              " 'number',\n",
              " 'departure',\n",
              " 'datetime',\n",
              " 'location',\n",
              " 'arrival',\n",
              " 'datetime',\n",
              " 'location',\n",
              " 'flight',\n",
              " 'airline',\n",
              " 'flight',\n",
              " 'number',\n",
              " 'departure',\n",
              " 'datetime',\n",
              " 'location',\n",
              " 'arrival',\n",
              " 'datetime',\n",
              " 'location',\n",
              " 'local',\n",
              " 'attractions',\n",
              " 'attraction',\n",
              " 'description',\n",
              " 'ticket',\n",
              " 'require',\n",
              " 'yn',\n",
              " 'attraction',\n",
              " 'description',\n",
              " 'ticket',\n",
              " 'require',\n",
              " 'yn',\n",
              " 'optional',\n",
              " 'travel',\n",
              " 'budgeting',\n",
              " 'planned',\n",
              " 'budget',\n",
              " 'flight',\n",
              " 'accommodation',\n",
              " 'food',\n",
              " 'attraction',\n",
              " 'miscellaneous',\n",
              " 'actual',\n",
              " 'spending',\n",
              " '업데이트',\n",
              " '시',\n",
              " '입력',\n",
              " 'travel',\n",
              " 'documents',\n",
              " 'passport',\n",
              " 'number',\n",
              " 'expiry',\n",
              " 'date',\n",
              " 'visa',\n",
              " 'type',\n",
              " 'expiry',\n",
              " 'date',\n",
              " 'national',\n",
              " 'id',\n",
              " 'driver',\n",
              " 'license',\n",
              " 'refugee',\n",
              " 'travel',\n",
              " 'document',\n",
              " 'advance',\n",
              " 'parole',\n",
              " 'document',\n",
              " 'reentry',\n",
              " 'permit',\n",
              " 'temporary',\n",
              " 'protected',\n",
              " 'status',\n",
              " 'travel',\n",
              " 'authorization',\n",
              " 'note',\n",
              " '기타',\n",
              " '메모',\n",
              " '작성',\n",
              " '여행',\n",
              " '중',\n",
              " '느낀',\n",
              " '점',\n",
              " '사진',\n",
              " '링크',\n",
              " '등',\n",
              " '기록',\n",
              " '이',\n",
              " '템플릿은',\n",
              " '여행을',\n",
              " '시작하기',\n",
              " '전에',\n",
              " '정보를',\n",
              " '입력하고',\n",
              " '여행',\n",
              " '중',\n",
              " '및',\n",
              " '후에',\n",
              " '업데이트하며',\n",
              " '활용할',\n",
              " '수',\n",
              " '있도록',\n",
              " '구성되어',\n",
              " '있습니다',\n",
              " 'obsidian에서',\n",
              " 'wikilinks로',\n",
              " '다른',\n",
              " '관련',\n",
              " '노트와',\n",
              " '연결하면',\n",
              " '더욱',\n",
              " '유용합니다']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1d46c05"
      },
      "source": [
        "# Task\n",
        "Perform a comprehensive text analysis of the Korean text stored in the `test` variable, which originates from the file `'/content/drive/MyDrive/MetaCogAI/the-verdict.txt'`. This analysis should include:\n",
        "1.  **Text Preprocessing**: Implement sentence segmentation, morpheme analysis using KoNLPy, remove stopwords (including a custom list), and clean the text by removing special characters, numbers, and emojis.\n",
        "2.  **Basic Text Analysis**: Calculate and visualize word frequencies, and generate a word cloud to highlight prominent terms.\n",
        "3.  **Advanced Text Analysis**: Apply Latent Dirichlet Allocation (LDA) for topic modeling to identify and extract key themes.\n",
        "4.  **Interpretation**: Visualize the identified topics and their keywords, then provide insights based on the word frequencies, word cloud, and topic modeling results.\n",
        "5.  **Summarize** the entire text analysis process and its key findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f991e6c"
      },
      "source": [
        "## Setup and Load Text\n",
        "\n",
        "### Subtask:\n",
        "Install necessary Korean NLP libraries (like KoNLPy) and ensure the input text from the 'test' variable is ready for processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98f5a206"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to install `konlpy` and `jpype1` to fulfill the first instruction of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43a2f3e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45659a45-ae2a-4f54-e633-e6c35bd0c35d"
      },
      "source": [
        "pip install konlpy jpype1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting jpype1\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from jpype1) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jpype1, konlpy\n",
            "Successfully installed jpype1-1.6.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad8b6f6e"
      },
      "source": [
        "**Reasoning**:\n",
        "The next instruction is to verify the installation of KoNLPy by importing a tagger, such as `Okt`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6fc20e1",
        "outputId": "af437a46-8bac-4b4d-d684-56811a92c5a2"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "print(\"KoNLPy's Okt tagger imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KoNLPy's Okt tagger imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7f0cd1c"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Perform sentence segmentation, morpheme analysis (tokenization) using KoNLPy, remove stopwords (with a custom list), and clean the text by removing special characters, numbers, and emojis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03db18f"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the `re` module, define custom stopwords and desired POS tags, perform sentence segmentation, and then iterate through each sentence to apply morpheme analysis, filter by POS, remove stopwords, clean characters, and filter by length, finally collecting all valid tokens into a list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3cff996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4038521-4836-486d-e496-27f4045a3844"
      },
      "source": [
        "import re\n",
        "\n",
        "# 2. Define custom Korean stopwords\n",
        "custom_stopwords = [\n",
        "    '이다', '있다', '하다', '되다', '이다', '것', '이', '그', '저', '수', '때', '말',\n",
        "    '밖', '개', '원', '점', '요', '음', '씨', '분', '곳', '알', '나', '내', '들',\n",
        "    '좀', '더', '줄', '게', '듯', '뿐', '채', '안', '영', '고', '면', '로', '에', '와',\n",
        "    '과', '을', '를', '은', '는', '도', '만', '이', '가', '의', '며', '서', '구', '과',\n",
        "    '며', '을', '와', '과', '하', '고', '되', '있', '받', '주', '아', '보', '같', '에', '에서',\n",
        "    '으로', '에게', '에서', '부터', '까지', '으로서', '으로', '처럼', '만큼', '뿐', '이라도', '라도',\n",
        "    '마저', '조차', '커녕', '거나', '든지', '다', '자', '고', '니까', '니까', '다', '습니다', 'ㅂ니다',\n",
        "    '다', '까', '습니다', 'ㄹ까요', 'ㅂ시다', '읍시다', '아', '어요', '아요', '오', '이요', '세요', '시오'\n",
        "]\n",
        "\n",
        "# 3. Define desired part-of-speech tags\n",
        "desired_pos_tags = ['Noun', 'Verb', 'Adjective', 'Adverb']\n",
        "\n",
        "# List to store all preprocessed tokens\n",
        "preprocessed_tokens = []\n",
        "\n",
        "# Apply okt.pos directly to the entire 'test' variable for morpheme analysis\n",
        "# This implicitly treats the whole text as a single unit for tokenization.\n",
        "pos_tags_for_text = okt.pos(test, norm=True, stem=True)\n",
        "\n",
        "# Iterate through the morphemes and their part-of-speech tags\n",
        "for morpheme, pos_tag in pos_tags_for_text:\n",
        "    # b. For each (morpheme, pos_tag) pair:\n",
        "    # i. Check if the pos_tag is in your defined list of desired POS tags.\n",
        "    if pos_tag in desired_pos_tags:\n",
        "        # ii. Convert the morpheme to lowercase.\n",
        "        morpheme_lower = morpheme.lower()\n",
        "\n",
        "        # iii. Check if the morpheme is not in your custom stopword list.\n",
        "        if morpheme_lower not in custom_stopwords:\n",
        "            # iv. Remove any numbers, special characters, or emojis\n",
        "            # using regular expressions for Korean/English characters.\n",
        "            cleaned_morpheme = re.sub(r'[^가-힣a-zA-Z]', '', morpheme_lower)\n",
        "\n",
        "            # v. Ensure the cleaned morpheme is at least 2 characters long\n",
        "            if len(cleaned_morpheme) >= 2:\n",
        "                preprocessed_tokens.append(cleaned_morpheme)\n",
        "\n",
        "print(f\"Total preprocessed tokens: {len(preprocessed_tokens)}\")\n",
        "print(f\"First 20 preprocessed tokens: {preprocessed_tokens[:20]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total preprocessed tokens: 675\n",
            "First 20 preprocessed tokens: ['음악', '미술', '숙련', '과정', '모방', '필요하다', '음악', '미술', '예술', '경지', '가다', '자기', '만들다', '이러하다', '창작', '음악', '작곡', '미술', '드러나다', '완전하다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_tokens"
      ],
      "metadata": {
        "id": "hf8w4RuOuAgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aba1b1a"
      },
      "source": [
        "## Build Analysis Data\n",
        "\n",
        "### Subtask:\n",
        "Create a Document-Term Matrix using TF-IDF (Term Frequency-Inverse Document Frequency) from the preprocessed text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb9627e"
      },
      "source": [
        "**Reasoning**:\n",
        "To create a TF-IDF matrix, I need to import the `TfidfVectorizer` class, join the preprocessed tokens into a single document string, then instantiate, fit, and transform the vectorizer, and finally extract the feature names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d08ceb4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Join the preprocessed_tokens list into a single string\n",
        "single_document = ' '.join(preprocessed_tokens)\n",
        "\n",
        "# Instantiate TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the single document to create the TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([single_document])\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(f\"TF-IDF Matrix shape: {tfidf_matrix.shape}\")\n",
        "print(f\"Number of features (unique words): {len(feature_names)}\")\n",
        "print(f\"First 10 feature names: {feature_names[:10]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe531df0"
      },
      "source": [
        "## Basic Text Analysis\n",
        "\n",
        "### Subtask:\n",
        "Calculate and visualize the frequency of words. Generate and display a word cloud to highlight the most prominent terms in the text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ecb983"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin the basic text analysis, I will import necessary libraries (`collections`, `pandas`, `matplotlib.pyplot`, `seaborn`), calculate word frequencies from `preprocessed_tokens` using `collections.Counter`, convert the results into a sorted Pandas DataFrame, print the top 10 most frequent words, and then visualize the top 15 words with a bar plot using `seaborn` as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9da588"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous plot suffered from missing Korean glyphs due to an unrefreshed font cache after installation. I will improve the font installation and caching process, then generate and display a word cloud as the next step in basic text analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad0a817c"
      },
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# Ensure Korean font is installed and set for matplotlib\n",
        "try:\n",
        "    # Check if font is already available or installed in the current session\n",
        "    if 'NanumBarunGothic' not in [f.name for f in fm.fontManager.ttflist]:\n",
        "        print(\"NanumBarunGothic font not found, installing...\")\n",
        "        !apt-get update -qq > /dev/null\n",
        "        !apt-get install -qq -y fonts-nanum > /dev/null\n",
        "        # Add font to matplotlib's font manager and rebuild cache\n",
        "        fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf')\n",
        "        fm._load_fontmanager(clear=True)\n",
        "        print(\"NanumBarunGothic font installed and cache rebuilt.\")\n",
        "    else:\n",
        "        print(\"NanumBarunGothic font already available.\")\n",
        "\n",
        "    plt.rc('font', family='NanumBarunGothic')\n",
        "    plt.rcParams['axes.unicode_minus'] = False # Fix for negative signs in Korean font\n",
        "    print(\"Matplotlib font set to NanumBarunGothic.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting NanumBarunGothic font: {e}. Using default font.\")\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_counts = collections.Counter(preprocessed_tokens)\n",
        "\n",
        "# Convert to DataFrame and sort\n",
        "word_freq_df = pd.DataFrame(word_counts.most_common(), columns=['word', 'frequency'])\n",
        "\n",
        "# Print top 10 most frequent words\n",
        "print(\"\\nTop 10 Most Frequent Words:\")\n",
        "print(word_freq_df.head(10))\n",
        "\n",
        "# Create a bar plot for the top 15 most frequent words\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='word', y='frequency', data=word_freq_df.head(15), palette='viridis')\n",
        "plt.title('Top 15 Most Frequent Words')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Join all the tokens into a single string for the word cloud\n",
        "words_for_wordcloud = ' '.join(preprocessed_tokens)\n",
        "\n",
        "# Specify a font path that supports Korean characters for the word cloud\n",
        "# Use the same NanumBarunGothic font or another common path if available\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(\n",
        "    font_path=font_path,\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white'\n",
        ").generate(words_for_wordcloud)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Preprocessed Text')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이제 글 한편 단위 토큰빈도분석이 끝났으니, 이제 여러 글 단위 토큰빈도분석을 수행해보자. 그것을 사용자가 글감 및 글쓰기 진행을 수월하게 하도록 '로고스 소프트웨어'의 '관주' 기능처럼 추천해주도록 하자.\n",
        "\n"
      ],
      "metadata": {
        "id": "McJjj2Gf031R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "033de233"
      },
      "source": [
        "# Task\n",
        "Preprocess all documents from the `documents` list by applying tokenization, POS tagging, stopword removal (using the existing `custom_stopwords` list), and cleaning to each document, then convert them into a list of preprocessed document strings. Afterwards, create a TF-IDF vectorizer using these preprocessed document strings to generate a document-term matrix for all documents, and then implement a document recommendation system that calculates cosine similarity between a user's input and the document TF-IDF vectors, returning the most similar document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eabf96a"
      },
      "source": [
        "## Preprocess All Documents for Vectorization\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the `documents` list, apply tokenization, POS tagging, stopword removal, and cleaning to each document. Collect all processed tokens for each document into a list of token lists, then join them into strings suitable for TF-IDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0887aa81"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to preprocess each document in the `documents` list by tokenizing, POS tagging, removing stopwords, and cleaning, then join the processed tokens for each document into a single string and collect them into a new list named `preprocessed_documents_for_vectorization`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22778de3"
      },
      "source": [
        "preprocessed_documents_for_vectorization = []\n",
        "\n",
        "for doc in documents:\n",
        "    doc_tokens = []\n",
        "    pos_tags_for_doc = okt.pos(doc, norm=True, stem=True)\n",
        "\n",
        "    for morpheme, pos_tag in pos_tags_for_doc:\n",
        "        if pos_tag in desired_pos_tags:\n",
        "            morpheme_lower = morpheme.lower()\n",
        "            if morpheme_lower not in custom_stopwords:\n",
        "                cleaned_token = re.sub(r'[^가-힣a-zA-Z]', '', morpheme_lower)\n",
        "                if len(cleaned_token) >= 2:\n",
        "                    doc_tokens.append(cleaned_token)\n",
        "\n",
        "    preprocessed_documents_for_vectorization.append(' '.join(doc_tokens))\n",
        "\n",
        "print(f\"Total number of preprocessed documents: {len(preprocessed_documents_for_vectorization)}\")\n",
        "print(f\"First 3 preprocessed document strings:\\n{preprocessed_documents_for_vectorization[:3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33de7ff3"
      },
      "source": [
        "## Create TF-IDF Vectors for All Documents\n",
        "\n",
        "### Subtask:\n",
        "Initialize and fit a `TfidfVectorizer` on the list of preprocessed document strings. Transform each document string into a TF-IDF vector, creating a matrix where each row represents a document's TF-IDF vector. Store this matrix and the fitted vectorizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f26b714"
      },
      "source": [
        "**Reasoning**:\n",
        "To create the TF-IDF vectors for all documents, I will initialize a `TfidfVectorizer`, fit it on the `preprocessed_documents_for_vectorization` list, transform the documents to get the TF-IDF matrix, and then print its shape and the number of features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e30c561"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Initialize a TfidfVectorizer object.\n",
        "tfidf_vectorizer_all_docs = TfidfVectorizer()\n",
        "\n",
        "# 2. Fit the TfidfVectorizer on the preprocessed_documents_for_vectorization list.\n",
        "# 3. Transform the preprocessed_documents_for_vectorization list using the fitted vectorizer\n",
        "#    to obtain the TF-IDF matrix.\n",
        "document_tfidf_matrix = tfidf_vectorizer_all_docs.fit_transform(preprocessed_documents_for_vectorization)\n",
        "\n",
        "# Get the feature names (words) for all documents\n",
        "feature_names_all_docs = tfidf_vectorizer_all_docs.get_feature_names_out()\n",
        "\n",
        "# 4. Print the shape of the resulting document_tfidf_matrix and the number of features.\n",
        "print(f\"TF-IDF Matrix shape for all documents: {document_tfidf_matrix.shape}\")\n",
        "print(f\"Number of features (unique words) for all documents: {len(feature_names_all_docs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65b8bf16"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the TF-IDF matrix for all documents has been created, the next step is to implement the document recommendation system. This involves preprocessing a user's input, transforming it into a TF-IDF vector using the previously fitted vectorizer, calculating cosine similarity with all documents, and then identifying the most similar document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f02f9a62"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def recommend_document(user_input, vectorizer, document_matrix, documents_list, top_n=1):\n",
        "    \"\"\"\n",
        "    Recommends documents based on cosine similarity to a user's input.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The text input from the user.\n",
        "        vectorizer: The fitted TfidfVectorizer.\n",
        "        document_matrix: The TF-IDF matrix of all documents.\n",
        "        documents_list (list): The original list of document strings.\n",
        "        top_n (int): The number of top similar documents to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (document_index, similarity_score) for the top_n similar documents.\n",
        "    \"\"\"\n",
        "    # Preprocess user input using the same steps as the documents\n",
        "    user_tokens = []\n",
        "    pos_tags_for_user_input = okt.pos(user_input, norm=True, stem=True)\n",
        "\n",
        "    for morpheme, pos_tag in pos_tags_for_user_input:\n",
        "        if pos_tag in desired_pos_tags:\n",
        "            morpheme_lower = morpheme.lower()\n",
        "            if morpheme_lower not in custom_stopwords:\n",
        "                cleaned_token = re.sub(r'[^가-힣a-zA-Z]', '', morpheme_lower)\n",
        "                if len(cleaned_token) >= 2:\n",
        "                    user_tokens.append(cleaned_token)\n",
        "\n",
        "    preprocessed_user_input = ' '.join(user_tokens)\n",
        "\n",
        "    # Transform the preprocessed user input into a TF-IDF vector\n",
        "    user_input_tfidf = vectorizer.transform([preprocessed_user_input])\n",
        "\n",
        "    # Calculate cosine similarity between the user input and all documents\n",
        "    cosine_similarities = cosine_similarity(user_input_tfidf, document_matrix).flatten()\n",
        "\n",
        "    # Get the indices of the most similar documents\n",
        "    most_similar_indices = cosine_similarities.argsort()[:-top_n-1:-1]\n",
        "\n",
        "    recommendations = []\n",
        "    for idx in most_similar_indices:\n",
        "        recommendations.append({\n",
        "            'document_index': idx,\n",
        "            'similarity_score': cosine_similarities[idx],\n",
        "            'original_document_snippet': documents_list[idx][:200] + '...'\n",
        "        })\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Example usage:\n",
        "user_query = '옥시토신'# \"예술 창작 모방 경험 중요성\"\n",
        "recommendations = recommend_document(user_query, tfidf_vectorizer_all_docs, document_tfidf_matrix, documents)\n",
        "\n",
        "print(\"Recommended Documents:\")\n",
        "for rec in recommendations:\n",
        "    print(f\"Document Index: {rec['document_index']}, Similarity Score: {rec['similarity_score']:.4f}\")\n",
        "    print(f\"Snippet: {rec['original_document_snippet']}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc50dc1f"
      },
      "source": [
        "## Test Recommendation System\n",
        "\n",
        "### Subtask:\n",
        "Provide a sample user input, preprocess it, use the `recommend_document` function to find the most similar document, and print the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19661bb6"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   All 208 raw documents were successfully preprocessed into cleaned, tokenized strings. This involved morpheme analysis using `okt.pos` with normalization and stemming, filtering by `desired_pos_tags`, converting to lowercase, removing `custom_stopwords`, cleaning with regex to remove non-Korean/English characters, and ensuring tokens were at least two characters long.\n",
        "*   A `TfidfVectorizer` was successfully initialized and fitted on the preprocessed documents, resulting in a `document_tfidf_matrix` with a shape of (208, 7258). This indicates 208 documents and 7258 unique features (words) identified across the corpus.\n",
        "*   A document recommendation system was implemented using the `recommend_document` function. This function preprocesses user input using the same methodology as the documents, transforms it into a TF-IDF vector, and calculates cosine similarity against the `document_tfidf_matrix`.\n",
        "*   For the example user query \"예술 창작 모방 경험 중요성\", the system successfully recommended Document Index 4 as the most similar, with a cosine similarity score of 0.4132.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The established TF-IDF based recommendation system provides a solid baseline for content-based recommendations, effectively matching user queries to relevant documents based on lexical similarity.\n",
        "*   To enhance the recommendation quality, future work could explore incorporating semantic similarity methods (e.g., word embeddings, topic modeling), considering document metadata, or implementing A/B testing to evaluate user satisfaction with the recommendations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전체 문서txt합들에 대해 토큰화하고 상위빈도토큰을 벡터공간에 넣어서 코사인유사도 기반으로 사용자 쿼리에 대한 상위빈도토큰을 매치하는 작업(a)을 성공했다. 이제 워드프로세서에서 작업 중인 파일에 대해서 드래그를 하거나 작업중인 문서 전체를 읽고 사용자가 요청하면 a작업을 수행하는 기능을 만들어보자."
      ],
      "metadata": {
        "id": "gu1gJIdxDl94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 작업 중인 문서에 대한 추천 시작 ---\\n\")\n",
        "\n",
        "# 사용자(작업자)가 워드프로세서에서 작성 중인 문서 내용을 시뮬레이션합니다.\n",
        "# 실제 워드프로세서에서 드래그하거나 읽어오는 과정은 Colab 환경에서 직접 구현하기 어렵기 때문에,\n",
        "# 임의의 텍스트를 user_document_in_progress 변수에 할당하는 방식으로 대체합니다.\n",
        "user_document_in_progress = \"음악과 미술 창작에 있어서 모방 경험이 숙련도를 높이는 데 중요하다. 완전한 모방은 예술의 경지로 나아가는 필수적인 과정이다.\"\n",
        "\n",
        "# 기존에 구현된 recommend_document 함수를 사용하여 유사한 문서를 추천합니다.\n",
        "# 이 함수는 텍스트를 전처리하고 TF-IDF 벡터로 변환하여 코사인 유사도를 계산합니다.\n",
        "recommendations_for_in_progress = recommend_document(\n",
        "    user_document_in_progress,\n",
        "    tfidf_vectorizer_all_docs,\n",
        "    document_tfidf_matrix,\n",
        "    documents\n",
        ")\n",
        "\n",
        "print(\"작업 중인 문서와 가장 유사한 추천 문서:\")\n",
        "for rec in recommendations_for_in_progress:\n",
        "    print(f\"문서 인덱스: {rec['document_index']}, 유사도 점수: {rec['similarity_score']:.4f}\")\n",
        "    print(f\"원본 문서 스니펫: {rec['original_document_snippet']}\")\n",
        "\n",
        "print(\"\\n--- 추천 완료 ---\\n\")"
      ],
      "metadata": {
        "id": "r2C7PKzOEDyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}